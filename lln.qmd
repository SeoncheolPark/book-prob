# The Law of Large Numbers {#sec-lln}

- 확률변수의 합에 대한 성질 알아보기

- 큰 수의 법칙: 확률변수의 열 $\{X_i\}_{i=1}^n$과 그 합 $S_n = \sum_{i=1}^n X_i$를 생각하고, $n\rightarrow\infty$일 때 확률변수열의 극한에 대해 살펴보는 것

- (개인적인 생각) LLN은 관련된 응용 개념이 확률론에서 많이 등장하므로 그런 측면에서 중요해 보이기도 함

## Preliminaries

제일 많이 쓰이는 기술은 **truncation**이라고 하는 것으로, 이 방법의 특징은 원래 확률변수열과 asymptotically equivalent 하면서 좀 더 다루기 쉬운 수열을 생각하는 것이다.

::: {#def-truncation}
## Truncation

(@Durrett2019 2.2.3)

To **truncate** a random variable $X$ at level $M$ means considering
$$
\overline{X} = XI_{(\vert X \vert \leq M)}=
\begin{cases}
X & \text{if } \vert X \vert \leq M\\
0 & \text{if } \vert X \vert > M
\end{cases}.
$$

:::

::: callout-tip
## Remark

예를 들어 finite second moment 없이 weak law를 하려면 truncation으로 자르고 Chebyshev's inequality를 적용하면 된다고 한다.
:::

## Convergence Equivalence

**Q**. 두 개의 random variable의 sequence $\{X_n\}$과 $\{Y_n\}$의 같은 정도? 비슷한 정도는?

::: {#def-convequiv}
## Convergence equivalence

The sequences $X_1, X_2, \ldots$ and $Y_1, Y_2, \ldots$ of random variables are said to be **convergence equivalent** if
$$
\sum_{n=1}^{\infty}P(X_n \neq Y_n) < \infty.
$$

:::

다음 정리는 first Borel-Cantelli lemma @thm-firstborelcantelli 로부터 얻는다고 한다. [@Gut2014]

::: {#thm-convequivthm1}

If $X_1, X_2, \ldots$ and $Y_1, Y_2, \ldots$ are converge equivalent, then:

1. $P(X_n \neq Y_n \text{ i.o.})=0$

2. $\sum_{n=1}^{\infty}(X_n - Y_n)$ converges a.s.

3. If $b_n \in \mathbb{R}$, $n\geq 1$, $b_n \uparrow \infty$ as $n\rightarrow\infty$, then
$$
\frac{1}{b_n} \sum_{k=1}^n (X_k - Y_k) \stackrel{\text{a.s.}}{\rightarrow}0 \quad{} \text{as }n\rightarrow\infty.
$$

:::

::: callout-tip
## Remark

다음 예제는 truncation을 이용해 어떤 확률변수열 $\{X_n\}$의 convergence equivalent한 새로운 함수열을 만들 수 있음을 보인다.
:::

::: {#exm-truncconvequiv}

## Truncation과 convergence equivalent

If $X_1, X_2,\ldots$ is such that $\sum_{n=1}^{\infty} P (\vert X_n \vert >a)<\infty$, then we obtain a convergence equivalent sequence by introducing the random variables $Y_n = X_n I \{\vert X_n \vert  \leq a \}$.

:::

## Moments and Tails

::: {#prp-momentsandtails}
## Moments and Tails

1.  Let $r>0$. Suppose that $X$ is a non-negative random variable. Then $$
    EX^r < \infty \quad{}\Longrightarrow \quad{} x^r P(X>x) \rightarrow 0 \quad{} \text{as }x \rightarrow \infty,
    $$ but not necessarily conversely.

2.  Suppose that $X, X_1, X_2, \ldots$ are i.i.d. random variables with mean $0$. Then, for any $a >0$, $$
    EXI\{ |X| \leq a\}= - EXI\{ |X|>a\},
    $$ and $$
    \Big| E \sum_{k=1}^n X_k I\{ |X_k| \leq a\} \Big|\leq n E|X| I\{|X|>a\}.
    $$

3.  Let $a >0$. If $X$ is a random variable with mean $0$, then $Y=XI\{|X|\leq a\}$ does not in general have mean $0$. However, if $X$ is **symmetric**, then $EY=0$.
:::

## A Weak Law for Partial Maxima

```{=html}
<!--
::: panel-tabset
## SNU, 2017 Winter, Problem 3 {.unlisted .unnumbered}

Suppose that $X_n$ are i.i.d. r.v.'s with $EX_1 >0$ and $S_n = X_1 + \cdots + X_n$. Show that $\max_{1\leq k \leq n} |X_k|/\sqrt{n} \rightarrow 0$ a.s.

(a) $\{|X_n|^r\}$ is uniformly integrable.

(b) $X_n \rightarrow X$ in $\mathcal{L}^r$.

(c) $E(|X_n|^r) \rightarrow E(|X|^r) < \infty$.

## Solution {.unlisted .unnumbered}

@Gut2014 책에는 almost sure convergence에만 초점을 맞추어 서술하였다. (Theorem 5.5.2) 실제 converge in probability에서도 성립함이 알려져 있다. (Theorem 5.5.4) 이 증명을 하기 위해서는 Fatou's lemma가 필요하다.
:::
-->
```

## The Weak Law of Large Numbers

<!--
정의: Gut의 책, 확률변수론
-->

::: {#thm-WLLN}
## WLLN

- $X,X_1,X_2,\ldots$가 유한한 기댓값 $\mu$, 즉 $E(X)<\infty$를 갖는 i.i.d. 확률변수들이라고 하고 $S_n,n\geq 1$을 그들의 partial sum이라고 하자. 그러면
$$
\frac{S_n}{n}\stackrel{p}{\rightarrow}\mu \quad{} \text{as } n\rightarrow \infty.
$$

<!--
Probability and Statistics for Economists 7.6
-->

::: callout-tip
## Remark

- $E|X|< \infty$ 조건이 빠지게 되면 표본평균이 유한한 값에 확률수렴하지 않을 수 있음
:::

:::

::: {#exm-notbip}
## 큰 수의 약법칙을 따르지 않는 확률변수열

- 독립이고 한계분포가 모두 $\text{Cauchy}(1,0)$인 확률벡터를 생각하면, $\frac{S_n}{n}\sim \text{Cauchy}(1,0)$이라고 함

- 즉 $\frac{S_n}{n}$이 0에 확률수렴하지 않으므로 독립이고 분포가 같은 코시 확률변수열에서는 큰 수의 약한 법칙이 성립하지 않음

:::


<!--
Counterexamples in prob 15장
-->

::: {#thm-markovthm}
## Markov theorem

$\{X_n, n\geq 1\}$을 다음의 조건 (**Markov condition**)
$$
\frac{1}{n^2} \text{Var}(X_1 + \cdots + X_n) \rightarrow 0 \quad{} \text{as }n \rightarrow \infty
$$
을 만족하는 확률변수열이라 하자. 그러면 $\{X_n\}$은 WLLN을 만족한다.

:::

::: callout-tip
## Remark

- 마르코프 조건은 WLLN에 대한 충분조건이나 필요조건은 아님
:::

## A Weak Law Without Finite Mean

::: {#thm-WLLNwithoutfinite}
## A Weak law without finite mean

(@Gut2014 의 6.4, @Durrett2019 의 Theorem 2.2.12)

Suppose that $X, X_1, X_2,\ldots$ are i.i.d. random variables with partial sums $S_n, n\geq 1$. Then
$$
\frac{S_n - n E(X I\{\vert X\vert \leq n\})}{n} \stackrel{p}{\rightarrow} 0 \quad{} \text{as } n \rightarrow\infty
$$
iff
$$
n P(\vert X \vert > n) \rightarrow 0 \quad{} \text{as } n \rightarrow \infty.
$$

:::



## The Strong Law of Large Numbers



::: {#thm-SLLNsuff}
## 큰 수의 센 법칙을 따르는 충분조건

확률변수열 $\{x_i\}_{i=1}^n$의 평균이 $\mu_i$, 분산이 $\sigma_i^2$일 때 
$$
\sum_{i=1}^{\infty} \sigma_i^2 < \infty
$$
이면 $\sum_{i=1}^{\infty}(X_i - \mu_i)$는 거의 틀림없이 0으로 수렴한다.


:::

::: callout-tip
## Remark

- 큰 수의 센 법칙을 따르는 확률변수열 은 (당연히) 큰 수의 약한 법칙도 성립함
:::

<!--
Gut책 295쪽
-->

::: {#thm-SLLNKolmogorov}
## The Kolmogorov strong law

(a) 만약 $E|X|<\infty$이고 $E(X)=\mu$라면
$$
\frac{S_n}{n}\stackrel{\text{a.s.}}{\rightarrow}\mu \quad{} \text{as } n \rightarrow \infty.
$$

(b) 만약 어떤 상수 $c$에 대해 $\frac{S_n}{n}\stackrel{\text{a.s.}}{\rightarrow} c$ as $n\rightarrow \infty$라고 한다면
$$
E(X) <\infty \quad{} \text{and} \quad{} c= E(X).
$$

(c) 만약 $E|X| = \infty$라 한다면
$$
\lim\sup_{n\rightarrow\infty} \frac{|S_n|}{n} = + \infty.
$$

:::


::: {#exm-WLLNnotSLLN}
## WLLN은 성립하니 SLLN은 만족하지 않는 예

확률질량함수 $p_{X_n}(x) = P\{ X_n = x\}$가
$$
p_{X_n}(x) =
\begin{cases}
1- \frac{1}{n\log n}, & x=0,\\
\frac{1}{2n\log n}, & x = \pm n
\end{cases}
$$
인 독립 확률변수열 $\{X_n\}_{n=2}^{\infty}$를 생각하자.

- $n \geq 2$일 때 $A_n = \{|X_n| \geq n\}$이라고 두면 $P\{A_n\} = \frac{1}{n\log n}$이므로 $\sum_{n=2}^{\infty} P\{A_n\} \rightarrow \infty$이다. 바꾸어 말하면, $\sum_{n=2}^{\infty}P\{A_n\}$이 발산하고 $X_n$이 서로 독립이므로, 보렐-칸텔리 정리에서 $P\{A_n \text{ i.o.}\}=1$이다. 이는
$$
P\{|X_n|\geq n \text{ i.o.}\} = P \{ \Big\vert\frac{X_n}{n}\Big\vert \geq \text{ i.o.} \} = P\{\lim_{n\rightarrow\infty}\frac{S_n}{n} \neq 0 \} =1
$$
이다. 그러므로 $\{X_n\}_{n=2}^{\infty}$는 큰 수의 강법칙을 만족하지 않는다.

- $\text{Var}\{X_ k\} = \frac{k}{\log k}$이므로 
$$
\begin{align*}
\frac{1}{n^2}\sum_{k=2}^n \text{Var}\{X_k\} &\leq \frac{1}{n^2} \Big( \frac{2}{\log 2} + \int_3^{n+1}\frac{x}{\log x} dx \Big)\\
&\leq \frac{2}{n^2 \log 2} + \frac{(n-2)(n+1)}{n^2 \log n}\\
&\rightarrow 0
\end{align*}
$$
이고 마르코프 조건 @thm-markovthm 을 생각해 보면 $\{X_n \}_{n=2}^{\infty}$는 큰 수의 약법칙을 만족한다.

:::

<!--
::: callout-tip
## Remark
-->

<!--
https://math.stackexchange.com/questions/2024255/what-is-the-difference-between-the-weak-and-strong-law-of-large-numbers
-->

<!--
WLLN과 SLLN을 대략 다음과 같이 쓸 수 있다.
$$
\begin{cases}
\text{WLLN}: \lim_{n\rightarrow\infty} P( |\overline{X}_n - E[X]|<\varepsilon)=1\\
\text{SLLN}: P(\lim_{n\rightarrow\infty}  |\overline{X}_n - E[X]|=0)=1\\
\end{cases}
$$

- WLLN에서는 sample size에 따른 확률의 sequence $\{P_1, P_2, \ldots, P_n\}$을 생각했을 때, 매우 작은 $\varepsilon>0$을 고르더라도 sample mean과 population mean 사이의 차이 ($|\overline{X}_n - E[X]|$)가 $\varepsilon$보다 작을 확률이 $n\rightarrow \infty$임에 따라 1로 수렴한다는 뜻

- SLLN에서는 sample mean과 population mean 사이의 차이가 0이 될 확률이 1로 수렴한다는 뜻

:::
-->

## The Glivenko–Cantelli Theorem

Let $X_1, X_2, \ldots, X_n$ be a sample from the distribution $F$.

- Empirical distribution function
$$
F_{n}(x) = \frac{1}{n}\sum_{k=1}^n I\{X_k \leq x\}.
$$

[@Gut2014] Strong law로부터 모든 $x$에 대해
$$
F_n(x) \stackrel{\text{a.s.}}{\rightarrow} F(x),\quad{} \text{as }n\rightarrow\infty.
$$

::: callout-tip
## Remark

Glivenko-Cantelli Theorem은 $n\rightarrow\infty$일 때 $F_n$이 $F$에 대해 converges uniformly한다는 것을 보여준다. [@Durrett2019]
:::

::: {#thm-glivenkocantelli}

## The Glivenko-Cantelli theorem

$$
\sup_x \vert F_n (x) - F(x) \vert \stackrel{\text{a.s.}}{\rightarrow} 0 \quad{} \text{as }n \rightarrow \infty.
$$

:::

::: {.callout-note collapse="true"}
## Proof

- Fix $x$ and let $Y_n = I_{(X_n \leq x)}$. Since $Y_n$ are i.i.d. with $E(Y_n) = P(X_n \leq x) = F(x)$, the SLLN implies that $F_n (x) = \frac{1}{n}\sum_{m=1}^n Y_m \stackrel{\text{a.s.}}{\rightarrow} F(x)$.

- In general, if $F_n$ is a sequence of nondecreasing functions that converges pointwise to a bounded and **continuous** limit $F$, then $\sup_x \vert F_n (x) - F(x) \vert \rightarrow 0$.

- 그러나 보통 $F(x)$는 jump를 가질 수 있고, 이때의 증명은 조금 복잡해진다. [@Durrett2019]

- Fix $x$ and let $Z_n =  I_{(X_n \leq x)}$. Since $Z_n$ are i.i.d. with $E(Z_n) = P(X_n < x) = F(x-) = \lim_{y\uparrow x} F(y)$, the SLLN implies that $F_n (x-) = \frac{1}{n}\sum_{m=1}^n Z_m \rightarrow F(x-)$ a.s. For $1 \leq j \leq k-1$, let $x_{j,k} = \inf \{y: F(y)\geq j/k\}$. The pointwise convergence of $F_n(x)$ and $F_n(x-)$ imply that we can pick $N_k (\omega)$ so that if $n \geq N_k (\omega)$, then
$$
\vert F_n (x_{j,k}) - F(x_{j,k}) \vert \frac{1}{k} \quad{} \text{and} \quad{} \vert F_n (x_{j,k}-) - F(x_{j,k}-) \vert < \frac{1}{k}
$$
for $1 \leq j \leq k-1$. If we let $x_{0,k} = - \infty$ and $x_{k,k} = \infty$, then the last two inequalities hold for $j=0$ or $k$. If $x \in (x_{j-1, k}, x_{j,k})$ with $1\leq j \leq k$ and $n\geq N_k (\omega)$, then using the monotonicity of $F_n$ and $F$, and $F(x_{j,k}-) - F(x_{j-1},k) \leq \frac{1}{k}$, we have
$$
\begin{align*}
F_n (x) &\leq F_n (x_{j,k}-) \leq F(x_{j,k}-) + \frac{1}{k}\leq F(x_{j-1,k}) + \frac{2}{k} \leq F(x) + \frac{2}{k}\\
F_n (x) & \geq F_n (x_{j-1,k}) \geq F(x_{j-1,k}) - \frac{1}{k} \geq F(x_{j,k}-) - \frac{2}{k} \geq F(x) - \frac{2}{k}
\end{align*}
$$
so $\sup_x \vert F_n(x) - F(x) \vert \leq \frac{2}{k}$, and we have proved the result.

:::

### Glivenko-Cantelli theorem의 R 예제

- 출처: [Computer Intensive Statistics: APTS 2023–24 Computer Practical 1 Solutions](https://warwick.ac.uk/fac/sci/statistics/apts/students/resources/prac1sol.pdf)


```{r}
#| echo: true
#| message: false
#| fig-align: center
#| fig-cap: "Figure: Illustration of Glivenko–Cantelli."
#| out-width: 70%
#| fig-height: 6
#| fig-weight: 6
#| view-distance: 10

ru <- runif(10000); F10 <- ecdf(ru[1:10]); F50 <- ecdf(ru[1:50]) 
F500 <- ecdf(ru[1:500]); F3000 <- ecdf(ru[1:3000]); F10000 <- ecdf(ru)

eu <- c(10, max(abs(F10(ru[1:10]) - punif(ru[1:10]))))
eu <- rbind(eu, c(50, max(abs(F50(ru[1:50]) - punif(ru[1:50])))))
eu <- rbind(eu, c(500, max(abs(F500(ru[1:500]) - punif(ru[1:500])))))
eu <- rbind(eu, c(3000, max(abs(F3000(ru[1:3000]) - punif(ru[1:3000])))))
eu <- rbind(eu, c(10000, max(abs(F10000(ru[1:10000]) - punif(ru[1:10000])))))

rn <- rnorm(10000); Fn10 <- ecdf(rn[1:10]); Fn50 <- ecdf(rn[1:50]) 
Fn500 <- ecdf(rn[1:500]); Fn3000 <- ecdf(rn[1:3000]); Fn10000 <- ecdf(rn)

en <- c(10, max(abs(Fn10(rn[1:10]) - pnorm(rn[1:10]))))
en <- rbind(en, c(50, max(abs(Fn50(rn[1:50]) - pnorm(rn[1:50])))))
en <- rbind(en, c(500, max(abs(Fn500(rn[1:500]) - pnorm(rn[1:500])))))
en <- rbind(en, c(3000, max(abs(Fn3000(rn[1:3000]) - pnorm(rn[1:3000])))))
en <- rbind(en, c(10000, max(abs(Fn10000(rn[1:10000]) - pnorm(rn[1:10000])))))

rc <- rcauchy(10000); Fc10 <- ecdf(rc[1:10]); Fc50 <- ecdf(rc[1:50]) 
Fc500 <- ecdf(rc[1:500]); Fc3000 <- ecdf(rc[1:3000]); Fc10000 <- ecdf(rc)

ec <- c(10, max(abs(Fc10(rc[1:10]) - pcauchy(rc[1:10]))))
ec <- rbind(ec, c(50, max(abs(Fc50(rc[1:50]) - pcauchy(rc[1:50])))))
ec <- rbind(ec, c(500, max(abs(Fc500(rc[1:500]) - pcauchy(rc[1:500])))))
ec <- rbind(ec, c(3000, max(abs(Fc3000(rc[1:3000]) - pcauchy(rc[1:3000]))))) 
ec <- rbind(ec, c(10000, max(abs(Fc10000(rc[1:10000]) - pcauchy(rc[1:10000])))))

plot(eu,type='l',col='red',main='Convergence of Empirical CDF')
lines(en,col='blue')
lines(ec,col='green')
legend('topright',c('Uniform','Normal','Cauchy'), lty=1, col=c('red','blue','green'))
```